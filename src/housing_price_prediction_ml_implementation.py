# -*- coding: utf-8 -*-
"""Housing_Price_Prediction_ML_Implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H5ODhz2pyedqPtaQVJbp55RhDK8dN5s4
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os

"""## Reading The Cleaned Dataset"""
dirname = os.path.dirname(__file__)
csv_path = os.path.join(dirname, '../Data/boston_house_prices.csv')
df = pd.read_csv(csv_path)
dataset=df[df.columns[:-1]]
dataset['Price'] = df[df.columns[-1]]

## Independent and Dependent features
X=dataset.iloc[:,:-1]
y=dataset.iloc[:,-1]

##Train Test Split
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)

## Standardize the dataset
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
X_train=scaler.fit_transform(X_train)
X_test=scaler.transform(X_test)
import pickle
pickle.dump(scaler,open('scaling.pkl','wb'))

"""## Model Training with MLFlow Logging"""


from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
import mlflow

"""Linear Regression"""

from sklearn.linear_model import LinearRegression
with mlflow.start_run(run_name="simple_linear_regression"):
    regression=LinearRegression()
    regression.fit(X_train,y_train)
    ## on which parameters the model has been trained
    regression.get_params()

    ### Prediction With Test Data
    reg_pred=regression.predict(X_test)

    ## Residuals
    residuals=y_test-reg_pred

    mae = mean_absolute_error(y_test,reg_pred)
    mse = mean_squared_error(y_test,reg_pred)
    rmse = np.sqrt(mean_squared_error(y_test,reg_pred))

    mlflow.log_metric("mae", mae)
    mlflow.log_metric("mse", mse)
    mlflow.log_metric("rmse", rmse)

    mlflow.sklearn.log_model(regression, "simple_linear_regression_model")

"""Neural Network"""

import mlflow
import mlflow.keras
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.metrics import MeanAbsoluteError
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.model_selection import train_test_split

# Define the neural network model
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(64, activation='relu'),
    Dense(1)  # Assuming a regression problem with a single output
])

# Compile the model
model.compile(optimizer=Adam(), loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])

# Start an MLflow run
with mlflow.start_run(run_name="simple_neural_network"):
    # Train the model
    history = model.fit(X_train, y_train, epochs=50, validation_split=0.2, verbose=1)

    # Make predictions
    y_pred = model.predict(X_test)

    # Calculate metrics
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)

    # Log metrics
    mlflow.log_metric("mse", mse)
    mlflow.log_metric("mae", mae)

    # Log the model
    mlflow.keras.log_model(model, "simple_neural_network_model")

    print(f"Model logged with MSE: {mse} and MAE: {mae}")

"""XGBoost"""

import xgboost as xgb

# Start an MLflow run
with mlflow.start_run(run_name="xgboost_regression"):

    # Define the model
    model = xgb.XGBRegressor(
        objective='reg:squarederror',  # For regression tasks
        n_estimators=100,              # Number of boosting rounds
        learning_rate=0.1,             # Step size shrinkage
        max_depth=5                    # Maximum depth of trees
    )

    # Train the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Calculate metrics
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)

    # Log metrics
    mlflow.log_metric("mse", mse)
    mlflow.log_metric("mae", mae)

    # Log the model
    mlflow.xgboost.log_model(model, "xgboost_model")

    print(f"Mean Squared Error: {mse}")
    print(f"Mean Absolute Error: {mae}")

"""## Hyperparameter Tuning with GridSearchCV"""

from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import r2_score
import json

"""Linear Regression with Hyperparameter Tuning and MLFlow Logging"""

# Define the model
model = Ridge()

# Define the parameter grid
param_grid = {
    'alpha': [0.1, 1.0, 10.0, 100.0],
    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']
}

# Save the parameter grid to a JSON file
param_grid_filename = 'regression_param_grid.json'
with open(param_grid_filename, 'w') as f:
    json.dump(param_grid, f)

# Set up GridSearchCV
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')

# Start an MLflow run
with mlflow.start_run(run_name="ridge_regression_grid_search"):
    # Perform grid search
    grid_search.fit(X_train, y_train)

    # Best model from grid search
    best_model = grid_search.best_estimator_

    # Make predictions
    y_pred = best_model.predict(X_test)

    # Calculate metrics
    mae = mean_absolute_error(y_test,y_pred)
    mse = mean_squared_error(y_test,y_pred)
    rmse = np.sqrt(mean_squared_error(y_test,y_pred))

    score=r2_score(y_test,reg_pred)
    print(score)

    # Log parameters
    mlflow.log_params(grid_search.best_params_)

    # Log metrics
    mlflow.log_metric("mse", mse)
    mlflow.log_metric("mae", mae)

    mlflow.log_artifact(param_grid_filename)

    # Log each combination of hyperparameters
    for i in range(len(grid_search.cv_results_['params'])):
        with mlflow.start_run(run_name=f"grid_search_{i}", nested=True):
            params = grid_search.cv_results_['params'][i]
            mean_test_score = -grid_search.cv_results_['mean_test_score'][i]
            std_test_score = grid_search.cv_results_['std_test_score'][i]

            mlflow.log_params(params)
            mlflow.log_metric("mse", mean_test_score)
            mlflow.log_metric("mae", std_test_score)


    # Log the model
    mlflow.sklearn.log_model(best_model, "ridge_regression_model")

    print(f"Best parameters: {grid_search.best_params_}")
    print(f"Model logged with MSE: {mse}")

"""XGBoost Hyper Parameter Tuning"""

import xgboost as xgb
import mlflow
import mlflow.xgboost
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Generate or load data
X = np.random.rand(100, 10)
y = np.random.rand(100)

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2]
}

# Define the model
model = xgb.XGBRegressor(objective='reg:squarederror')

# Define GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)

# Start an MLflow run
with mlflow.start_run(run_name="xgboost_grid_search"):

    # Fit GridSearchCV
    grid_search.fit(X_train, y_train)

    # Get the best model
    best_model = grid_search.best_estimator_

    # Make predictions
    y_pred = best_model.predict(X_test)

    # Calculate metrics
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)

    # Log the best parameters and metrics for this trial
    mlflow.log_params(grid_search.best_params_)
    mlflow.log_metric("mse", mse)
    mlflow.log_metric("mae", mae)

    # Log the best model
    mlflow.xgboost.log_model(best_model, "best_xgboost_model")

    print(f"Best parameters: {grid_search.best_params_}")
    print(f"Mean Squared Error: {mse}")
    print(f"Mean Absolute Error: {mae}")

    # Log each trial
    for i, (params, mean_score, _) in enumerate(zip(grid_search.cv_results_['params'], grid_search.cv_results_['mean_test_score'], grid_search.cv_results_['std_test_score'])):
        # Start a new MLflow run for each trial
        with mlflow.start_run(run_name=f"trial_{i}", nested=True):
            mlflow.log_params(params)
            mlflow.log_metric("mse", mean_test_score)
            mlflow.log_metric("mae", std_test_score)

"""## Pickling The Best Model file For Deployment"""

import pickle
pickle.dump(best_model,open('best_reg_model.pkl','wb'))
